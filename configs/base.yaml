max_history_len: 20
eval_ks: [10, 50]
val_metric: ndcg_10
num_workers: 4

UniSRec:
  max_seq_length: 20
  n_layers: 2
  n_heads: 2
  hidden_size: 300
  inner_size: 256
  hidden_dropout_prob: 0.5
  attn_dropout_prob: 0.5
  hidden_act: gelu
  layer_norm_eps: 1e-12
  initializer_range: 0.02
  loss_type: CE
  lambda: 1e-3
  train_stage: inductive_ft
  plm_size: 768
  adaptor_dropout_prob: 0.2
  adaptor_layers: [768, 300]
  temperature: 0.07
  n_exps: 8
  trainer:
    epochs: 300
    stopping_epochs: 40
    train_batch_size: 2048
    eval_batch_size: 2048
    optimizer: Adam
    lr: 0.001
    weight_decay: 0.0
    clip_grad_norm: null

RQ-VAE:
  sent_emb_dim: 768
  sent_emb_pca: null
  batch_size: 2048
  epochs: 10000
  lr: 0.001
  beta: 0.25
  hidden_dim: [2048, 1024, 512, 256, 128]
  num_layers: 3
  dropout: 0.0
  code_book_size: 256
  rqvae_low_usage_threshold: -1

TIGER:
  n_user_tokens: 1
  n_positions: 82
  n_layers: 4
  num_heads: 4
  d_model: 64
  d_ff: 1024
  d_kv: 64
  dropout_rate: 0.1
  activation_function: relu
  feed_forward_proj: relu
  trainer:
    total_steps: 200000
    optimizer: AdamW
    scheduler: cosine
    lr: 0.001
    warmup_steps: 10000
    weight_decay: 0.05
    train_batch_size: 256
    eval_batch_size: 32
  
SpecGR:
  temperature: 0.07
  projection: 64
  encoder_batch_size: 256

  pretrain_trainer:
    epochs: 80
    lambda_emb: 6.0
    lambda_gen: 1.0
    lr: 0.0003
    warmup_steps: 10000
    weight_decay: 0.035
    train_emb_batch_size: 1024
    train_gen_batch_size: 256
    eval_batch_size: 32

  finetune_trainer:
    epochs: 15
    lambda_emb: 20.0
    lambda_gen: 1.0
    lr: 3e-5
    warmup_steps: 10000
    reencode_interval: null
    weight_decay: 0.035
    train_batch_size: 256
    eval_batch_size: 32
